\documentclass[12pt]{amsart}

\pagestyle{empty}
\usepackage[margin=1.0in]{geometry}

\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\Rn}{\R^n}
\newcommand{\Rm}{\R^m}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\grad}{\nabla}
\newcommand{\Rnn}{\R^{n\times n}}
\newcommand{\map}[3]{#1:#2\rightarrow #3}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\tpose}[1]{#1^{\scriptscriptstyle T}}



\usepackage{amsmath,epsfig,amssymb}
%\input{macros}
%\input{macros2}

\begin{document}
{\Large Name: John Min, \Large UNI: jcm2199}  \\
\begin{center}
\Large COMS 4772 \hskip 2in Homework Set 1
\end{center}
\bigskip


Try to do all problems efficiently, but show key steps in your work. 

\noindent
{\sf Multi-variable Calculus Review Problems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
\item
Find the global minimizers, if they exist, for the following functions.
\begin{enumerate}
\item[(a)] $f(x)=x_1^2-4x_1+2x_2^2+7$

\noindent \\ 
\noindent
Solution: \\
$\frac{\partial f}{\partial x_1} = 2 x_1 - 4, \frac{\partial f}{\partial x_2} = 4 x_2$ \\
$\frac{\partial^2 f}{\partial x_1^2} = 2, \frac{\partial^2 f}{\partial x_1^2}$ \\
$ 2x_1^* - 4 = 0, 4x_2^* = 0 \Rightarrow x^* = (2,0)$. \\


\item[(b)] $f(x)=e^{-\norm{x}^2}$
\item[(c)] $f(x)=x_1^2-2x_1x_2+\frac{1}{3}x_2^3-4x_2$ \\

\noindent
Solution:\\
$\frac{\partial f}{\partial x_1} = 2x_1 - x_2 := 0 \Rightarrow x_2 = 2x_1 \\
\frac{\partial f}{\partial x_2} = -x_1 + x_2^2 - 4 := 0 \Rightarrow 4x_1^2 - x_1 - 4 = 0 \Rightarrow x_1^* = \pm \frac{\sqrt{17}}{8}$

$\frac{\partial^2 f}{\partial x_1^2} = 2, \frac{\partial^2 f}{\partial x_2^2} = 2x_2$ \\
\noindent
For our global minimum, we need $x_2 > 0$; hence, we choose $x^* = (\frac{\sqrt{17}}{8}, \frac{\sqrt{17}}{4})$.\\

\item[(d)] $f(x)=(2x_1-x_2)^2+(x_2-x_3)^2+(x_3-1)^2$\\

\noindent
Solution:\\
$f(x) = 4x_1^2 - 4x_1x_2 + 2x_2^2 - 2x_2x_3 + 2x_3^2 - 2x_3 + 1\\
\frac{\partial f}{\partial x_1} = 8x_1 - 4x_2 := 0 \Rightarrow 2x_1 = x_2\\
\frac{\partial f}{\partial x_2} = -4x_1 + 4x_2 - 2x_3 := 0 \Rightarrow x_2 - x_3 = 0 \Rightarrow x_2 = x_3\\
\frac{\partial f}{\partial x_3} = -2x_2 + 4x_3 - 2 :=0 \Rightarrow 2x_3 - 2 = 0 \Rightarrow x_3 = 1 \\
\frac{\partial^2 f}{\partial x_1^2} = 8, \frac{\partial^2 f}{\partial x_2^2} = 4, \frac{\partial^2 f}{\partial x_3^2} = 4 \\
x^* = (\frac{1}{2}, 1, 1)$ \\


\item[(e)] $f(x)=x_1^4+16x_1x_2+x_2^8$ \\

\noindent
Solution:\\
$\frac{\partial f}{\partial x_1} = 4x_1^3 + 16x_2 := 0 \Rightarrow x_1^3 + 4x_2 =0 \Rightarrow x_1^3 = -4x_2 \\
\frac{\partial f}{\partial x_2} = 16x_1 + 8x_2^7 = 0 \Rightarrow 2x_1 + x_2^7 = 0 \Rightarrow 2x_1 = -x_2^7 \\
\frac{\partial^2 f}{\partial x_1^2} = 12x_1^2, \frac{\partial^2 f}{\partial x_2^2} = 56x_2^6$.  Hence, the Hessian $\grad^2 f$ is semi-positive definite.

Solve the partial equations and $x^* = (-2^\frac{3}{4}, 2^\frac{1}{4})$. \\


\item[(f)] $f(x)=\sum_{j=1}^{n-1} 10^j(x_j-x_{j+1}^2)^2$ 
(The Rosenbrock function) \\

\noindent
Solution:\\
Since this function is a sum of squares, the global minimum is 0.  In this case, we see that if $x^* = (c, c^{1/2}, \ldots, c^{1/{2^{n-1}}})$, for $c \geq 0$ such that $x_i = c^{1/{2^{i-1}}}$ for $i = 1, \ldots, n$, then, we are at a global min.\\


\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item When $f$ is continuously differentiable at $x\in\Rn$,
then $\grad f(x)$ is easily computed as the vector of partial derivatives of $f$
at $x$.
%$$
%\grad f(x)=\left(\begin{array}{c}\frac{\partial f}{\partial x_1}(x)\\
%\frac{\partial f}{\partial x_2}(x)\\ \vdots \\
%\frac{\partial f}{\partial x_n}(x)
%\end{array}\right)\ .
%$$
Compute the gradient of the following functions.
\begin{enumerate}
\item[(a)] $f(x)=x_1^3+x_2^3-3x_1-15x_2+25 $\\

\noindent
Solution:\\
$\grad f(x) = [3x_1^2 - 3, 3x_2^2 - 15]$ \\

\item[(b)] $f(x)=x_1^2+x_2^2-\sin(x_1x_2)$ \\

\noindent
Solution:\\
$\grad f(x) = [2x_1 - cos(x_1 \cdot x_2) \cdot x_2, 2x_2 - cos(x_1 \cdot x_2) \cdot x_1]$ \\

\item[(c)] $f(x)=\norm{x}^2=\sum_{j=1}^nx_j^2$\\

\noindent
Solution:\\
$\grad f(x) = 2 \cdot [x_1, x_2, \ldots, x_n] $\\

\item[(d)] $f(x)=\displaystyle e^{\norm{x}^2}$\\

\noindent
Solution:\\
$\grad f(x) = 2f(x) \cdot [x_1, x_2, \ldots, x_n] = 2 \cdot \exp \{\displaystyle \sum_{j=1}^n x_j^2\} \cdot [x_1, x_2, \ldots, x_n]$\\


\item[(e)] $f(x)=x_1x_2x_3\cdots x_n $ \\

\noindent
Solution:\\
$\grad f(x) = f(x) \cdot [x_1, x_2, \ldots, x_n]^{-1} = x_1x_2x_3\cdots x_n \cdot [\frac{1}{x_1}, \frac{1}{x_2}, \ldots, \frac{1}{x_n}]$\\

\item[(f)] $f(x)=-\log(x_1x_2x_3\cdots x_n)$ for $x_j>0,\ j=1,\dots n$.\\

\noindent
Solution:\\
$\grad f(x) $\\

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\item
Let $\Rnn$ denote the set of real $n\times n$ square matrices
A function $\map{f}{\Rn}{\R}$ is said to be twice differentiable at a point $x\in\Rn$
if is differentiable at $x$ and there is a matrix $H\in\Rnn$ such that
$$f(y)=f(x)+\grad f(x)^T(y-x)+\half (y-x)^TH(y-x)+ o(\norm{y-x}^2).$$
The matrix $H$ is called the Hessian of $f$ at $x$ and is denoted $\grad^2f(x)$.
Note that, when defined, the relation $x\mapsto \grad^2 f(x)$ is
a mapping from $\Rn$ to $\Rnn$, i.e.~$\map{\grad^2 f}{\Rn}{\Rn}$.
We say that $f$ is twice continuously differentiable at $x\in\Rn$ if the mapping
$\grad^2 f$ is continuous at $x$. It can be shown that if $f$ is twice
continuously differentiable at a point $x\in\Rn$, then the matrix
$\grad^2f(x)$ is symmetric, i.e.~$\grad^2f(x)=\grad^2f(x)^T$, in which case
$\grad^2 f(x)$ is the matrix of second partial derivatives of $f$ at $x$:
$$
\grad^2f(x)=\left[\begin{array}{cccc}
\frac{\partial^2 f}{\partial x_1\partial x_1}(x)&
\frac{\partial^2 f}{\partial x_2\partial x_1}(x)&\dots&
\frac{\partial^2 f}{\partial x_n\partial x_1}(x)\\
\frac{\partial^2 f}{\partial x_1\partial x_2}(x)&
\frac{\partial^2 f}{\partial x_2\partial x_2}(x)&\dots&
\frac{\partial^2 f}{\partial x_n\partial x_2}(x)\\
\vdots&\vdots&\ddots&\vdots\\
\frac{\partial^2 f}{\partial x_1\partial x_n}(x)&
\frac{\partial^2 f}{\partial x_2\partial x_n}(x)&\dots&
\frac{\partial^2 f}{\partial x_n\partial x_n}(x)
\end{array}\right]\ .
$$
Compute the Hessian of the functions given in problem (2) above.

\begin{enumerate}
\item[(a)] $f(x)=x_1^3+x_2^3-3x_1-15x_2+25 $\\

\noindent
Solution:\\
$\grad^2 f(x) = 
\begin{bmatrix}
6x_1 & 0\\
0 	 & 6x_2
\end{bmatrix}
$ \\

\item[(b)] $f(x)=x_1^2+x_2^2-\sin(x_1x_2)$ \\

\noindent
Solution:\\
$\grad^2 f(x)= 
\begin{bmatrix}
2 + sin(x_1 x_2) \cdot x_2^2 & sin(x_1 x_2) \cdot x_1 x_2 - cos(x_1 x_2)\\
sin(x_1 x_2) \cdot x_1 x_2 - cos(x_1 \cdot x_2) & 2 + cos(x_1 x_2) \cdot x_1^2
\end{bmatrix}
$ \\

\item[(c)] $f(x)=\norm{x}^2=\sum_{j=1}^nx_j^2$\\

\noindent
Solution:\\
$\grad^2 f(x) = 2 \cdot \exp\{\displaystyle \sum_{j=1}^n x_j^2 \}
\begin{bmatrix}
2 & & & \\
& 2 & & \\
& & \ddots & \\
& & & 2\\
\end{bmatrix} $\\

\item[(d)] $f(x)=\displaystyle e^{\norm{x}^2}$\\

\noindent
Solution:\\
$\grad^2 f(x) = 
\begin{bmatrix}
x_1^2 + 1 & 2x_1 x_2 & &\\
		  & x_2^2 + 1 & &\\
& & \ddots & \\
& & & \ddots \\
& & & x_n^2 + 1
\end{bmatrix}
$\\


\item[(e)] $f(x)=x_1x_2x_3\cdots x_n $ \\

\noindent
Solution:\\
$\grad^2 f(x) = 
\begin{bmatrix}

\end{bmatrix}
$\\

\item[(f)] $f(x)=-\log(x_1x_2x_3\cdots x_n)$ for $x_j>0,\ j=1,\dots n$.\\

\noindent
Solution:\\
$\grad^2 f(x) =$\\
\end{enumerate}



\newpage

\noindent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\sf Working with Convex Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item
Show that each of the following functions is convex.
\begin{enumerate}
\item $f(x) = \begin{cases} 12 &\text{if}  \quad \|Ax - b\|_1 \leq \pi \\ \infty & \text{otherwise}\end{cases}$

\item
$f(x)=e^{-x}$
\item
$f(x_1,x_2,\dots,x_n)=e^{-(x_1+x_2+\dots +x_n)}$
\item
$f(x)=\norm{x}$ (Show for any norm, don't assume this is a particular norm). 
\item 
$f(x) = \log \left( \sum_i e^{x_i}\right)$
\end{enumerate}

\item
If $\map{f_i}{\Rn}{\R},\ i=1,2$ are convex, show that $f(x)=\max\{f_1(x),\ f_2(x)\}$
is a convex function. \\

\noindent
Solution: \\

Let $z = \lambda x + (1-\lambda) y$.
\begin{align*}
f(z) &= \max \{f_1(z), f_2(z)\} \\
        &= \max \{f_1(\lambda x + (1-\lambda) y), f_2(\lambda x + (1-\lambda) y)\}  \\
      &\leq \max \{\lambda f_1(x) + (1-\lambda) f_1(y), \lambda f_2(x) + (1-\lambda) f_2(y)\} \text{ by the convexity of } f_i\\
     &\leq \max \{\lambda \cdot \max\{f_1(x),\ f_2(x)\} + (1-\lambda) \cdot \max\{f_1(x),\ f_2(x)\}    \}\\
   & = \lambda \cdot \max\{f_1(x),\ f_2(x)\} + (1-\lambda) \cdot \max\{f_1(x),\ f_2(x)\}\\
   & = \lambda \cdot f(x) + (1-\lambda) \cdot f(y). \qed
\end{align*} 


\item
Let $A\in\R^{m\times n}$ and $b\in\Rm$, and suppose that
$\map{f}{\Rm}{\R}$ is convex. Show that $h(x)=f(Ax+b)$ is convex. \\

\noindent
Solution: \\

Let $z = \lambda x + (1-\lambda) y$.
\begin{align*}
h(z) &= f(Az + b)\\
&= f(A[\lambda x + (1-\lambda) y] + b)]\\
&= f(\lambda[Ax+b] + (1-\lambda)[Ay+b])\\
&\leq \lambda \cdot f(Ax+b) + (1-\lambda) \cdot f(Ay+b)\\
&= \lambda \cdot h(x) + (1-\lambda) \cdot h(y).  \qed
\end{align*}

\item
 Consider the linear equation $$Ax=b,$$ where $A \in \Rmn$
and $b \in \Rm$. When $n<m$ it is often the case that this
equation is over-determined in the sense that no solution $x$
exists. In such cases one often attempts to locate a `best'
solution in a least squares sense. That is one solves the
{\it linear least squares problem}
$$\hbox{(lls) : minimize } \half \norm{Ax-b}^2_2$$
for $x$. Define $\map{f}{\Rn}{\R}$ by
$$f(x):= \half \norm{Ax-b}^2_2.$$
\begin{enumerate}
\item Show that $f$ can be written as a quadratic function, i.e.
a function of the form
$$f(x):=\frac{1}{2} \tpose{x} Q x -\tpose{a}x +\alpha \  .$$

\noindent
Solution:\\
\begin{align*}
f(x) = \frac{1}{2} \norm{Ax-b}^2_2 &= \frac{1}{2}\norm{Ax-b}_2 \norm{Ax-b}_2 \\
&= \frac{1}{2} (Ax-b)^\top (Ax-b) \\
&= \frac{1}{2} \{(Ax)^\top (Ax) - b^\top (Ax) - (Ax)^\top b + b^\top b \} \\
&= \frac{1}{2} \{x^\top A^\top A x - 2 b^\top Ax + b^\top b \} 
\end{align*} \\

Set $Q = A^\top A, a = A^\top b, \alpha = \frac{1}{2} b^\top b$. \\

\item What are $\grad f(x)$ and $\grad^2 f(x)$? \\

\noindent
Solution: \\
$\grad f(x) = Qx - a = A^\top Ax - A^\top b = A^\top (Ax-b) \\
\grad^2 f(x) = Q = A^\top A$ \\

\item Show that $\grad^2 f(x)$ is positive semi-definite. \\

\noindent
Solution: \\
$ \forall z \in \mathbb{R}^n, z^\top \grad^2 f z = z^\top A^\top A z = (Az)^\top (Az) = \norm{Az}^2_2 \geq 0$.  \\
Hence, Q is P.S.D.  \\

\item$^*$ Show that a solution to (lls) must always exist.
\item$^*$ Provide a necessary and sufficient condition on the matrix
$A$ ({\bf not on the matrix $A^TA$}) under which (lls)
has a unique solution and then display this solution in terms
of the data $A$ and $b$.
\end{enumerate}
\item
Consider the functions
$$f(x)=\frac{1}{2} \tpose{x} Q x -\tpose{c}x $$
and
$$
f_t(x)=\frac{1}{2} \tpose{x} Q x -\tpose{c}x +t\phi(x),
$$
where $t>0$, $Q\in\R^{n\times n}$ is positive semi-definite,
$c\in\R^n$, and $\phi\, :\, \R^n\rightarrow\R\cup\{+\infty\}$
is given by
$$
\phi(x)=\left\{\begin{array}{ll}
-\sum_{i=1}^n\ln x_i&\mbox{, if $x_i> 0,\ i=1,2,\dots ,n$,}\\
+\infty&\mbox{, otherwise.}\end{array}\right.
$$
\begin{enumerate}
\item
Show that $\phi$ is a convex function. \\

\noindent
Solution:\\
The log function is concave, hence, the - log function is convex.  The sum of convex functions are convex. \\
$\grad_x \phi = -\frac{1}{x_i} = - x_i^{-1} \\
\grad_x^2 \phi = \text{ diag}(x_i^2)^{-1}$.\\
Clearly, positive definite as $x_i > 0$.  \\

\item
Show that both $f$ and $f_t$ are convex functions. \\

\noindent
Solution:\\
$\grad f(x) = Qx \\
\grad^2 f(x) = Q$, which is P.S.D. $\Rightarrow x^\top Qx$ is convex, $-c^\top x$ is linear, hence, convex. \\
f is convex, $\phi$ is convex $\Rightarrow f_t$ convex. \\

\item
Show that the solution to the problem $\min f_t(x)$ always
exists and is unique. \\

$\grad^2 f_t = \grad^2 f + \grad^2 \phi$ is strictly positive definite since $\grad_x^2 \phi$ is positive and $\grad^2 f = Q$ is semi-positive definite.\\
$\Rightarrow f_t$ is strictly convex.  \\
Hence, if the solution exists, then it is unique (and a solution exists, trivially). \\ 

\end{enumerate}



\newpage

\noindent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\sf Working with Convex Functions, part II.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item The conjugate of a convex function is defined by 
\[
f^*(y) = \sup_x x^Ty - f(x).
\]

Compute the conjugates of the following functions. 
\begin{enumerate}

\item $f(x) = \delta(x|C)$, where $C$ is a convex set.  \\

\noindent
Solution:\\
$f^*(x) = \displaystyle \sup_x x^\top y - \delta(x|C) = \sup_{x \in C} x^\top y$ \\

\item $f(x) = 3x^2 - 2x$ \\

\noindent
Solution:\\
$\frac{\partial f}{\partial x} = 0: y - 6\bar{x} + 2 = 0 \Rightarrow \bar{x} = \frac{1}{6} (y+2)$
\begin{align*}
f^*(y) &= \frac{1}{6}(y+2)y - \frac{1}{12}(y^2 + 4y + 4) + \frac{1}{3}y + \frac{2}{3}\\
         &= \frac{1}{12}(y^2 + 4y + 4) \\
	 &= \frac{1}{12} (y+2)^2
\end{align*}

\item $f(x) = x\log x$\\

\noindent
Solution:\\
$\frac{\partial f}{\partial x} = 0: y - log \bar{x} - 1 = 0 \Leftrightarrow log \bar{x} = y-1 \Leftrightarrow \bar{x} = \exp\{y-1\}$\\
\begin{align*}
f^*(y) &= \sup_x <x,y> - x log x \\
         &= y \cdot \exp\{y-1\} - (y-1) \exp\{y-1\} \\
	 &= \exp\{y-1\}
\end{align*}

\item $f(x) = \log(1 + e^x)$\\

\item $f(x) = e^{-2x}$.\\

\item $f(x) = \max_{1, \dots, n} x_i$. \\

\item $f(x) = x^p$ for $x > 0$ and $p > 1$. \\

\end{enumerate}

\item Conjugates in terms of other conjugates

\begin{enumerate}

\item Define $g(x) = f(x) + c^Tx + d$, where $f$ is convex. Express $g^*$ in terms of $f^*$, $c$, and $d$. \\

\noindent
Solution:
\begin{align*}
g^*(y) &= \sup_x x^\top y - f(x) - c^\top x - d \\
          &= \sup_x \{x^\top y - f(x) - c^\top x\} -d\\
	  &= \sup_x \{<x, y-c> - f(x)\} - d\\
          &= f^*(y-c) -d \\
\end{align*}

\item Define $g(x) = f(Qx + b)$, $Q$ an invertible matrix, and $b$ a vector. Compute $f^*$ in terms of $g^*$, $Q$, and $b$.\\

\noindent
Solution:\\
Let $u = Qx + b$.  $x = Q^{-1}(u-b)$.
\begin{align*}
g^*(y) &= \sup_u <u, Q^{-\top} y> - f(u) - b^\top Q^{-\top} y\\
          &= f^*(Q^{-\top}y) - (Q^{-1} b)^\top y
\end{align*}

\item Let $f(x,z)$ be convex in $(x,z)$ and define $g(z) = \inf_z f(x,z)$. Express $g^*$ in terms of $f^*$.\\

\noindent
Solution:\\
\begin{align*}
g^*(y) &= \sup_x <x,y> - \inf_z f(x,z)\\
          &= \sup_{x,z} \{<x,z> - f(x,z)\}\\
	  &= \sup_z \{sup_x <x,y> - f(x,z)\}\\
          &= \sup_z f^*(y)
\end{align*}
\end{enumerate}



\item Suppose that $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex, and bounded above on $\mathbb{R}^n$. Show that $f$ is constant. 


\vspace{2in}

{\bf Bonus: Don't work on this problem until you are done with the others. }

\item Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is convex and twice continuously differentiable. Suppose that $\bar y$ and $\bar x$ 
are related by $\bar y = \nabla f(\bar x)$, and that the hessian $\nabla^2 f(\bar x)$ is positive definite. 

\begin{enumerate}
\item  Show that $\nabla f^* (\bar y) = \bar x$. 

\item Show that $\nabla^2 f^* (\bar y) = \nabla^2 f(\bar x)^{-1}$.

\end{enumerate}

\end{enumerate}

\end{document}
